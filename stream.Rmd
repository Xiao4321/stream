---
title: "Stream Mining"
author: Bryan Paget, Xiao Liang, Xiaoshan Zhang
output:
  pdf_document:
    toc: true
    toc_depth: 2
fontsize: 11pt
date: "2018-12-01"
abstract : "We describe what stream mining is and how it is different from traditional data mining with static data sets.  We also perform some experiments on simulated streams of data.  A method for streaming real time financial data is also presented."
---

```{r setup, include = FALSE}
library(knitr)
library(tidyverse)
library(ggplot2)
library(stream)
library(quantmod)
library(knitr)

set.seed(42)

knitr::opts_chunk$set(echo = F,
                      warning = F,
                      error = F,
                      message = F,
                      dpi = 300,
                      fig.dim = c(4,4),
                      fig.align = 'center')
```

# Introduction

What differentiates stream mining from typical data mining?  It's the fact that we don't have all the data at the onset.  With typical forms of data mining, we are given a static set of data.  With stream mining, data points arrive one at a time and we do not know with certainty what the next data point will be, and in most applications, the stream is of infinite length.

# Issues Unique to Stream Mining

Data streams may deliver points rapidly and, depending on the application, it may be desirable to process the incoming data right away.  It may also be the case that since the data may be infinite, there is no storage, meaning the data can not be iterated over, which makes many traditional algorithms inappropriate for this kind of data mining.

**Definition (Data Stream):**
A *data stream* $d_s = (d_1, d_2, \dots)$ is an ordered, possibly unbounded, continuous sequence of data points generated by some unknown, and not necessarily stationary, stochastic process.

**Examples:**
Data streams may come in the following forms:

1. Medical sensor data including smart watches and exercise bands.
2. Environmental sensor data including smart thermostats.
3. Web click streams from social networking websites.
4. Blogging and micro-blogging posts.
5. Entries in an operating system's log files.
6. Music classification programs such as "Shazam".

**Definition (Stream Mining):** 
*Stream mining* is the process of extracting
useful information from a data stream in real-time, as the data becomes
available.

Since the data may be unbounded, the length of the stream may never cease, when
stream mining it is rare to want to store the data.  What is stored instead are
the statistics mind from the data.

Traditional data mining algorithms are not well suited to the evolving and
potentially speed and amount of change. Evaluation of stream mining algorithms
presents a unique challenge as the performance of the algorithm needs to be
evaluated in real-time, and the data may have underlying properties which evolve
over time, so that algorithm may start out performing well but then it may fail
to adapt to the stream. The model's capacity may fail to capture the data as it
evolves.

Since the data is continuously being created and mined, stream mining algorithms
only get to read the data once, unlike static data mining algorithms like
kmeans, EM etc... stream mining algorithms do not read the data multiple times.
How does this affect iterative methods?

Classification can be difficult since new classes may emerge from what were
originally labeled as outliers. One class may divide into two.

# Algorithms

## Classification

Data stream classification is very different from static data classification
since the whole data is never known, so we can not divide the data up into
train, test, and validation data sets.

Adapting to concept drift is key to a successful stream classification
algorithm.

### Performance Measures

#### Kappa

**Definition (Kappa Statistic):**
The *kappa statistic* $\kappa \in (0, 1)$ measures the performance of a data stream classifier by comparing its performance to a random classification algorithm that assigns random class labels to data points.  The kappa statistics is defined below

$$\kappa = {A_{train} - A_{random}\over 1 - A_{random}}$$

where $A_train$ is the accuracy of the classifier being trained and $A_random$ is the accuracy of assigning class labels at random.

#### Temporal Kappa

**Definition (Temporal Kappa Statistic):**
The *temporal kappa statistic* $\kappa_t \in (-\infty, 1)$ measures the tendency for the classifier to be influenced by the temporal ordering

$$\kappa_{t} = {A_{train} - A_{persistent}\over 1 - A_{persistent}}$$

where $A_{persistent}$ is the classifier which assigns the same label to data point $t+1$ as $t$ was labelled.

## Clustering

Number of clusters changes over time, and the boundaries of the clusters
themselves also change as new data arrives.

Clustering, the assignment of data points to groups such that points within each group are more similar than points in groups. For static data sets methods, which need access to all data points are unsuitable for data stream. We use a two-stage online/offline approach:

### Online

Summarize the data using a set of $k’$ micro-clusters, which are representatives for sets of similar data points and are created using a single pass over the data. Micro- clusters are typically represented by cluster centers, density and additional statistics. Each new data point is assigned to its closest micro-cluster, and if this data point cannot be assigned to an existing micro- cluster, then a new cluster is created. The algorithm may also keep the number of micro- clusters at a manageable size or the information up-to-date by deleting or merging micro-clusters.

### Offline

Use conventional clustering algorithm by regarding micro-clusters as pseudo-points since the offline part is usually not time critical. That means the $k’$ micro-clusters are reclustered into $k$ ($k << k'$) final clusters named macro-cluster.

### Performance Measures

##### Completeness

**Definition (Completeness):**
*Completeness* $\kappa \in (0, 1)$ measures the performance of a data stream classifier by comparing its performance to a random classification algorithm that assigns random class labels to data points.  The kappa statistics is defined below

$$\kappa = {A_{train} - A_{random}\over 1 - A_{random}}$$

where $A_train$ is the accuracy of the classifier being trained and $A_random$ is the accuracy of assigning class labels at random.


# Specific Algorithms

# 

### Stream (Available on CRAN)

```{R eval=F, echo=T}
stream <- DSD_Gaussians(k=3, noise=0)
kmeans <- DSC_Kmeans(k=3)
while (TRUE) {
  sample <- DSC_Sample(k=100)
  update(sample, stream, 100)
  recluster(kmeans, sample)
  plot(kmeans, stream, type="both", xlim=c(0, 1), ylim=c(0, 1))
}
```

# Concept Drift

To illustrate the phenomena of concept drift, we found the following toy data set, called `movingSquares` which contains points from four uniform distributions over square regions of $\mathbb{R}^2$.  The concept in question is the center point of the square.  As we play the stream, we can see they squares moving left and right in the plane.  If we use `kmeans` to find the center of each square, we have to adapt as new points become available.  In other words, the center of each square is not static, and we have to update the `kmeans` estimate, repeatedly.  The following code and animation illustrate concept drift and our method of adapting to it.

```{R eval=F, echo=T}
data.location <- "./Data/driftDatasets/artificial/movingSquares"
data <- read_delim(paste(data.location, "movingSquares.data", sep="/"),
                   col_names=F, delim = " ")
labels <- read_csv(paste(data.location, "movingSquares.labels", sep="/"), 
                   col_names=F) %>% 
  rename("Labels" = X1)
moving.squares <- cbind(data, labels)

kmeans <- DSC_Kmeans(k=4)
stream <- DSD_Memory(scale(moving.squares[,1:2], scale=T, center=T), 
                     class=moving.squares$Labels)

i <- 1
while (TRUE) {
  i <- i + 1
  sample <- DSC_Sample(k=40)
  update(sample, stream, 100)
  recluster(kmeans, sample)
  plot(kmeans, stream, type="both", xlim=c(-3, 3), ylim=c(-3, 3))
  Sys.sleep(time=0.1)
}
```

# Implementation

## Simulated Stream

We found the following 

```{R, eval=F}
if (!file.exists("./hist.png")) {
  ## This one is good.
  files <- list.files("./Data/Big/Stocks/")
  ## Need to know the smallest dataset, it's the limiting factor, but really I want to know the size of all the data sets:
  sizes <- list()
  for (file in files) {
    sizes[gsub('.{7}$', '', file)] <- dim(read_csv(paste("./Data/Big/Stocks", file, sep="/")))[1]
  }
  png("hist.png")
  hist(unlist(sizes), labels=T)
  dev.off()
}
```

[Histogram]("./hist.png")

```{R, eval=F}
if (!file.exists("./big.csv")) {
  historical.stock.data <- data.frame()
  for (file in files) {
    stock.data <- read_csv(paste("./Data/Big/Stocks", file, sep = "/"))
    if (dim(stock.data)[1] == 3201) {
      new <- stock.data %>%
        mutate("Company Name" = gsub('.{7}$', '', file),
               "Difference Day" = abs(Open - Close),
               "Difference Value" = abs(High - Close)) %>%
        filter(Date >= "2005-02-25" & Date <= "2017-11-10")
      historical.stock.data <- rbind(historical.stock.data, new)
    }
  }
  write_csv(historical.stock.data, path="./big.csv")
}

## Stocks that I like based on closing value or volatility
closing.value <- c("sgy", "mkl", "ghc", "wwr", "blk", "clsn", "isrg", "shpg", "lll")
volatility <- c("svra", "abio", "sgy", "scon", "tdw", "nvr", "bh", "ghc", "crs")

if (!file.exists("./sample.csv")) {
  stocks <- read_csv("./big.csv") %>% mutate(Date = as.factor(Date))
  levels(stocks$Date) <- 1:length(levels(stocks$Date))
  stocks %>% mutate(Date = as.numeric(Date))
sample <- (function(x, df) subset(df, `Company Name` %in% x)) (volatility, stocks)
write_csv(sample, path="./sample.csv")
}
```

## Animated Demo of Stream

To demonstrate stream clustering, we'll want to pick out some stocks that are different from each other.

```{R, eval=F, echo=T}
sample <- read_csv("./sample.csv")
ordered <- sample[order(sample$Date),] %>% 
  mutate(`Company Name` = as.factor(`Company Name`))
reset.stream <- function(Variable)
{
  DSD_Memory(ordered %>% select(Date, Variable), class=ordered$`Company Name`)
}
simulated.stream <- reset.stream("Close")
```

## Animated Demo of Stream

```{R, echo=T, eval=F}
make.animation <- function(Variable, df)
{
  i <- 1
  while (i < dim(df)[1]) {
    i <- i + 9
    my.points <- df[i:(i+9),]
    plot(c(), xlab="Day", ylab=Variable, xlim=c(0, dim(df)[1]/9), ylim=c(0, 2000))
    points(x=my.points$Date, y=my.points[[Variable]], col=my.points$`Company Name`)
  }
}
make.animation("Close", ordered)

simulated.stream <- reset.stream("Close")
animate_data(simulated.stream, horizon = 9, n = 3201*9, wait = .1, plot.args = list(shared=TRUE), loop=TRUE)
```

[GIF animate](http://slides.yihui.name/gif/car-transform.gif)

From the large, concatenated dataset, we can create a simulated data stream and use the kmeans algorithm built into `streams`.
## TODO : find the labels of the points that belong to the clusters.
```{R, echo=T, eval=F}
stream <- reset.stream("Close")
kmeans <- DSC_Kmeans(k=2)
assignments <- c()
for (i in 1:100) {
  my.sample <- DSC_Sample(k=40)
  update(my.sample, stream, 40)
  recluster(kmeans, my.sample)
  for (i in 1:length(kmeans$RObj$assignment)) {
    assignments <- c(assignments, kmeans$RObj$assignment[[i]]) 
  }
  plot(kmeans, stream, n=i*40, type="both", xlim=c(0, 28809), ylim=c(0, 5000))
  Sys.sleep(1)
}
```

## Real Stream

Many data-centric businesses offer APIs to access their data. Twitter for
instance has an API, but one must request access to the API. We applied for
access, but have not heard back from Twitter. We then decided to go with the `R`
package `quantmod`, which has an interface to access real time stock prices. The
following code-snippet runs an infinite loop to grab a quote from the General
Electric stock price.  GE was chosen because it was listed as a relatively
volatile stock compared with other stocks.  The code grabs a price every 15
seconds and adds it to a plot.

```{R echo=T, eval=F}
i = 0
plot(c(), xlim=c(0, 100), ylim=c(0, 1000), xlab="X", ylab="Y", main="Stocks")
while (TRUE) {
    quotes <- getQuote("GE;GOOG;AAPL")
    points(c(i, quotes$Last), pch=20, col=i)
    Sys.sleep(time=1)
    i <- i + 1
}
```

# Stream Queries
## TODO : record 

The following code will obtain the current stock price for general electric for 1000 iterations, each iteration separated by 15 seconds.  Once we have collected 1000 stock prices, we can later do offline analysis.  This demononstrates a method for collected a finite, discrete subset of a continuous and infinite length stream.  Notice the use of checkpoints, in case the program fails, the internet connection dies or the user of the script needs to stop it for some reason.

```{R eval=F}
dir.create("./checkpoints")
my.stocks <- tibble("Index", "Stock Price")
i = 0
while (i < 1000) {
  i <- i + 1
  quotes <- getQuote("GE")
  my.stocks <- rbind(my.stocks, c(date(), quotes$Last))
  if (i %% 10 == 0) {
    write_csv(interesting.points, 
              paste("./checkpoints/my.stocks.checkpoint.", i, ".csv", sep=""))
  }
  Sys.sleep(time=15)
}
write_csv(my.stocks, "./my.stocks.csv")
```

## Filtering a data stream

This ties in with the fact that the stream may be of uncountable cardinality and we are only interested in a very small subset that satisfy some criteria.

We can modify the above example, to record the time indices when the data points satisfy some criteria.  For example, we can look at the historical high and low for `GE`'s stock price, and we can set up an alert for when the stock goes above or below a certain value.  From looking at the NASDAQ's website, it looks like the average price of `GE` has been around \$7.50 for the past few days (as of Nov 30th).  The following code will record the time and date whenever the stock price leave the interval $[7.40, 7.60]$ and then export the `tibble` as a `csv`.

```{R echo=T, eval=F}
dir.create("./checkpoints")
interesting.points <- tibble("Index", "Stock Price")
i = 0
while (TRUE) {
  i <- i + 1
  quotes <- getQuote("GE")
  if (quotes$Last < 7.40 || quotes$Last > 7.60) {
    interesting.points <- rbind(interesting.points, c(date(), quotes$Last))
  }
  if (i %% 10 == 0) {
    write_csv(interesting.points, 
              paste("./checkpoints/interesting.points.checkpoint.", i, ".csv", sep=""))
  }
  Sys.sleep(time=15)
}
write_csv(interesting.points, "./interesting.points.csv")
```

I will have R watch the stream, and record values at each time step.  I can ask R to record the average value for every (let's say) 40 points. That one thing we can do with a stream.

```{R, echo=T, eval=F}
dir.create("./checkpoints")
points <- c()
while (TRUE) {
  points <- rbind(points, getQuote("GE"))
  if (length(points) %% 10 == 0) {
    cat(date(), "Mean:", mean(points))
  }
  Sys.sleep(time=15)
}
```

We can perform standing queries; we can ask `R` to keep track of the maximum or minimum values of the stock and update those values as new points become available.

```{R echo=T, eval=F}
min <- 0
max <- 0
while (TRUE) {
  quotes <- getQuote("GE")
  max <- ifelse(quotes$Last > max, quotes$Last, max)
  min <- ifelse(quotes$Last < min, quotes$Last, min)
  if (i %% 10 == 0) {
    cat("Max: ", max, ", Min: ", min, sep="")
  }
  Sys.sleep(time=15)
}
```

# Histogram to record number of recurrences of a specific element.

We can make a markov chain for a finite alphabet and we can generate a histogram for each letter.

```{R echo=T, eval=T}
letters <- c("A", "C", "G", "T")
A <- matrix(c(0.4, 0.3, 0.2, 0.1, 
              0.1, 0.4, 0.3, 0.2, 
              0.2, 0.1, 0.4, 0.3, 
              0.3, 0.2, 0.1, 0.4), nrow=4, ncol=4, byrow=T)
while(FALSE) {
  
}
```

# Summary

# References

1. [Stream Mining a Review : Tool and
   Techniques](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8212816)
2. [Introduction to stream: An Extensible Framework for Data Stream Clustering Research with R]()
3. Stream Data Mining: Platforms, Algorithms, Performance Evaluators and Research Trends
4. https://www.flickr.com/services/api/
5. https://www.quora.com/What-is-meant-by-streaming-API
6. stream R package
7. quantmod R package
8. https://moa.cms.waikato.ac.nz/datasets/
9. https://github.com/vlosing/driftDatasets
10. http://statisticalrecipes.blogspot.com/2013/01/easy-introduction-to-markov-chains-in-r.html