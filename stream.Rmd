---
title: "Stream Mining"
author: Bryan, Xiao, Shine
output:
  pdf_document:
    toc: true
    toc_depth: 2
    fig_caption: true
    df_print: kable
fontsize: 11pt
geometry: margin=1.5in
date: "2018-11-06"
fig_caption: true
abstract : "blah blah blah."
---

```{r setup, include = FALSE}
library(tidyverse)
library(ggplot2)

library(stream)
library(quantmod)

set.seed(42)

library(knitr)
knitr::opts_chunk$set(echo = F, warning = F, error = F, message = F,
                      dpi = 300, fig.dim = c(4,4), fig.align = 'center')
```

# Introduction

What differentiates stream mining from typical data mining?  It's the fact that we don't have all the data at the onset.  With typical forms of data mining, we are given a static set of data.  With stream mining, we are given one data point at a time and we do not know with certainty what the next data point will be.

**Definition (Data Stream):**
A *data stream* $d_s = (d_1, d_2, \dots)$ is an ordered, possibly unbounded, continuous sequence of data points generated by some unkown, and not necessarily stationary, stochastic process.

**Examples:**
Data streams may come in the following forms:

1. Medical sensor data including smart watches and exercise bands.
2. Environmental sensor data including smart thermostats.
3. Web click streams from social networking websites.
4. Blogging and microblogging posts.
5. Entries in an operating system's log files.
6. Shazam, 

**Definition (Stream Mining):** 
*Stream mining* is the process of extracting
useful information from a data stream in real-time, as the data becomes
available.

Since the data may be unbounded, the length of the stream may never cease, when
stream mining it is rare to want to store the data.  What is stored instead are
the statistics mind from the data.

Traditional data mining algorithms are not well suited to the evolving and
potentially speed and amount of change. Evaluation of stream mining algorithms
presents a unique challenge as the performance of the algorithm needs to be
evaluated in real-time, and the data may have underlying properties which evolve
over time, so that algorithm may start out performing well but then it may fail
to adapt to the stream. The model's capacity may fail to capture the data as it
evolves.

Since the data is continuously being created and mined, stream mining algorithms
only get to read the data once, unlike static data mining algorithms like
kmeans, EM etc... stream mining algorithms do not read the data multiple times.
How does this affect iterative methods?

Classification can be difficult since new classes may emerge from what were
originally labeled as outliers. One class may divide into two.

# Algorithms

## Classification

Data stream classification is very different from static data classification
since the whole data is never known, so we can not divide the data up into
train, test, and validation data sets.

Adapting to concept drift is key to a successful stream classification
algorithm.


### Performance Measures

#### Kappa

**Definition (Kappa Statistic):**
The *kappa statistic* $\kappa \in (0, 1)$ measures the performance of a data stream classifier by comparing its performance to a random classification algorithm that assigns random class labels to data points.  The kappa statistics is defined below

$$\kappa = {A_{train} - A_{random}\over 1 - A_{random}}$$

where $A_train$ is the accuracy of the classifier being trained and $A_random$ is the accuracy of assigning class labels at random.

#### Temporal Kappa

**Definition (Temporal Kappa Statistic):**
The *temporal kappa statistic* $\kappa_t \in (-\infty, 1)$ measures the tendency for the classifier to be influenced by the temporal ordering

$$\kappa_{t} = {A_{train} - A_{persistent}\over 1 - A_{persistent}}$$

where $A_{persistent}$ is the classifier which assigns the same label to data point $t+1$ as $t$ was labelled.

## Clustering

Number of clusters changes over time, and the boundaries of the clusters
themselves also change as new data arrives.

### Performance Measures

##### Completeness

**Definition (Completeness):**
*Completeness* $\kappa \in (0, 1)$ measures the performance of a data stream classifier by comparing its performance to a random classification algorithm that assigns random class labels to data points.  The kappa statistics is defined below

$$\kappa = {A_{train} - A_{random}\over 1 - A_{random}}$$

where $A_train$ is the accuracy of the classifier being trained and $A_random$ is the accuracy of assigning class labels at random.


# Specific Algorithms

### Stream (Available on CRAN)

```{R}
stream <- DSD_Gaussians(k=3, noise=0)
sample <- DSC_Sample(k=20)
update(sample, stream, 500)
sample
```

```{R}
kmeans <- DSC_Kmeans(k=3)
recluster(kmeans, sample)
plot(kmeans, stream, type="both")
```


## Comparison

Might want to look at some historical data, and show how the stream mining
adapted algorithms differ from the regular ones.

## Challenges

1. Infinite Length
  - Too much data to store, possibly coming in too frequently.

2. Concept Drift
  - As new points are made available, classification via separating hyperplane
    may need to be updated.
  - New features appear as new points become available.

3. Concept Evolution

4. Feature Evolution

# Implementation

## Simulated Stream

Since we want to produce a nice paper and presentation, we also worked with a simulated stream.  We found the following airline dataset, that is often used as a bench mark for stream mining algorithms.  These data need to be synchronized, if we simulate a multi-dimensional stream, we need a point for each dimension.  We can't have any "NULL" instances.

```{R}
files <- list.files("./Data/Big/Stocks/")
## Need to know the smallest dataset, it's the limiting factor, but really I want to know the size of all the data sets:
sizes <- list()
for (file in files) {
  sizes[gsub('.{7}$', '', file)] <- dim(read_csv(paste("./Data/Big/Stocks", file, sep="/")))[1]
}
hist(unlist(sizes), labels=T)

## There are many with 3201 entries
```


```{R}
## How many do we want?
if (!file.exists("./big.csv")) {
  historical.stock.data <- data.frame()
  for (file in files) {
    stock.data <- read_csv(paste("./Data/Big/Stocks", file, sep = "/"))
    if (dim(stock.data)[1] == 3201) {
      new <- stock.data %>%
        mutate("Company Name" = gsub('.{7}$', '', file),
               "Difference Day" = abs(Open - Close),
               "Difference Value" = abs(High - Close)) %>%
        filter(Date >= "2005-02-25" & Date <= "2017-11-10")
      historical.stock.data <- rbind(historical.stock.data, new)
    }
  }
  write_csv(historical.stock.data, path="./big.csv")
}
stocks <- read_csv("./big.csv")

```

To demonstrate stream clustering, we'll want to pick out some stocks that are different from each other.

```{R}
summary <- stocks %>% 
  group_by(`Company Name`) %>% 
  summarize_all(mean)
tail(summary)

stocks <- stocks %>% mutate(Date = as.factor(Date))
levels(stocks$Date) <- 1:length(levels(stocks$Date))
stocks %>% mutate(Date = as.numeric(Date))
```

```{R}
closing.value <- c("sgy", "mkl", "ghc", "wwr", "blk", "clsn", "isrg", "shpg", "lll")

volatility <- c("svra", "abio", "sgy", "scon", "tdw", "nvr", "bh", "ghc", "crs")

create.sample <- function(desired, df)
{
  subset(df, `Company Name` %in% desired)
}
sample <- create.sample(volatility, stocks)
  
write_csv(sample, path="./sample.csv")

sample <- read_csv("./sample.csv")
ordered <- sample[order(sample$Date),] %>% mutate(`Company Name` = as.factor(`Company Name`))

reset.stream <- function(Variable)
{
  DSD_Memory(ordered %>% select(Date, Variable), class=ordered$`Company Name`)
}
simulated.stream <- reset.stream("Difference Day")
```

## Animated Demo of Stream

```{R}
ordered %>% group_by(Date) %>% tally()

make.animation <- function(Variable, df)
{
  plot(c(), xlab="Day", ylab=Variable, xlim=c(0, dim(df)[1]/9), ylim=c(0, 2000))
  i <- 1
  while (i < dim(df)[1]) {
    i <- i + 9
    my.points <- df[i:(i+9),]
    points(x=my.points$Date, y=my.points[[Variable]], col=my.points$`Company Name`)
  }
}
make.animation("Difference Day", ordered)

simulated.stream <- reset.stream("Difference Day")
animate_data(simulated.stream, horizon = 9, n = 3201*9, wait = .1, plot.args = list(shared=TRUE), loop=TRUE)
```

From the large, concatenated dataset, we can create a simulated data stream and use the kmeans algorithm built into `streams`.

```{R}
dsd <- reset.stream("Difference Day")

dsd <- reset.stream("Close")

dsc <- DSC_Kmeans(k=2)
animate_cluster(dsc, dsd, horizon=162, n=3201*9)
```

## Real Stream

Many data-centric businesses offer APIs to access their data. Twitter for
instance has an API, but one must request access to the API. We applied for
access, but have not heard back from Twitter. We then decided to go with the `R`
package `quantmod`, which has an interface to access real time stock prices. The
following code-snippet runs an infinite loop to grab a quote from the General
Electric stock price.  GE was chosen because it was listed as a relatively
volatile stock compared with other stocks.  The code grabs a price every 15
seconds and adds it to a plot.

```{R echo=TRUE, eval=FALSE}
i = 0
plot(c(), xlim=c(0, 100), ylim=c(0, 1000), xlab="X", ylab="Y", main="Stocks")
while (TRUE) {
    quotes <- getQuote("GE;GOOG;AAPL")
    points(quotes$Last, pch=20, col=i)
    Sys.sleep(time=15)
    i <- i + 1
}
```

The problem we had with the above stream, is that we were not sure what to do with the stream.

# Summary

# References

1. [Stream Mining a Review : Tool and
   Techniques](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8212816)
2. [Introduction to stream: An Extensible Framework for Data Stream Clustering Research with R]()
3. Stream Data Mining: Platforms, Algorithms, Performance Evaluators and Research Trends
4. https://www.flickr.com/services/api/
5. https://www.quora.com/What-is-meant-by-streaming-API
6. stream R package
7. quantmod R package
8. https://moa.cms.waikato.ac.nz/datasets/
